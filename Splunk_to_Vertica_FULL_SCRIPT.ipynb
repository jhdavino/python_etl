{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connect to Splunk...\n",
      "\n",
      "Running Splunk query...\n",
      "\n",
      "67.7%   2545 scanned   2545 matched   4139 results\n",
      " Splunk query Done!\n",
      "\n",
      "\n",
      "DataFrame Ready...\n",
      "\n",
      "Field astype config\n",
      "\n",
      "Stream init...\n",
      "\n",
      "fdp_dashboards_temp_with_SF ...ok\n",
      "\n",
      "new_record - archived_record - overlap - total_records\n",
      "[[6, 64223, 4133, 68362]]\n",
      "\n",
      "\n",
      "fdp_dashboards_backup_with_SF ...ok\n",
      "\n",
      "fdp_dashboards_with_SF ...ok\n",
      "\n",
      "DONE :)\n",
      "\n",
      "Refresh Tableau Server...\n",
      "\n",
      "connection made\n",
      "\n",
      "2.8\n",
      "refresh complete\n",
      "\n",
      "connection closed\n",
      "\n",
      "DONE :)\n"
     ]
    }
   ],
   "source": [
    "# import splunklib as splunk\n",
    "import splunklib.client as client\n",
    "import splunklib.results as results\n",
    "import urllib.parse\n",
    "from pandas.io.sql import SQLTable\n",
    "\n",
    "def _execute_insert(self, conn, keys, data_iter):\n",
    "    data = [dict(zip(keys, row)) for row in data_iter]\n",
    "    conn.execute(self.table.insert().values(data))\n",
    "\n",
    "SQLTable._execute_insert = _execute_insert\n",
    "\n",
    "import pandas as pd\n",
    "import vertica_python\n",
    "import sqlalchemy as sa\n",
    "import pyodbc\n",
    "import sys\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import inspect\n",
    "from sqlalchemy.types import VARCHAR\n",
    "from sqlalchemy.types import Integer\n",
    "from time import sleep\n",
    "from pandas.io.sql import SQLTable\n",
    "import psycopg2\n",
    "import io #\n",
    "import tableauserverclient as TSC\n",
    "\n",
    "print(\"Connect to Splunk...\\n\")\n",
    "\n",
    "HOST = \"oprdfdshd300.corp.intuit.net\"    #connection param\n",
    "PORT = 8089\n",
    "USERNAME = \"douaknine\" # your username\n",
    "PASSWORD = 'Logorn12' # your password\n",
    "APP = \"ILAggregation\"\n",
    "\n",
    "service = client.connect(\n",
    "    host=HOST,\n",
    "    port=PORT,\n",
    "    username=USERNAME,\n",
    "    password=PASSWORD,\n",
    "    app=APP)\n",
    "\n",
    "\n",
    "print(\"Running Splunk query...\\n\")\n",
    "\n",
    "\n",
    "mysavedsearch = service.saved_searches[\"data_dashboards\"] #My Dashboard\n",
    "job = mysavedsearch.dispatch()\n",
    "\n",
    "\n",
    "while True:\n",
    "    while not job.is_ready():\n",
    "        pass\n",
    "    stats = {\"isDone\": job[\"isDone\"],\n",
    "             \"doneProgress\": float(job[\"doneProgress\"])*100,\n",
    "              \"scanCount\": int(job[\"scanCount\"]),\n",
    "              \"eventCount\": int(job[\"eventCount\"]),\n",
    "              \"resultCount\": int(job[\"resultCount\"])}\n",
    "\n",
    "    status = (\"\\r%(doneProgress)03.1f%%   %(scanCount)d scanned   \"\n",
    "              \"%(eventCount)d matched   %(resultCount)d results\") % stats\n",
    "\n",
    "    sys.stdout.write(status)\n",
    "    sys.stdout.flush()\n",
    "    if stats[\"isDone\"] == \"1\":\n",
    "        sys.stdout.write(\"\\n Splunk query Done!\\n\\n\")\n",
    "        break\n",
    "    sleep(2)\n",
    "\n",
    "# Create a Dataframe\n",
    "d = []            \n",
    "for result in results.ResultsReader(job.results(count = 0)):\n",
    "    d.append(result)\n",
    "\n",
    "job.cancel()   \n",
    "sys.stdout.write('\\n')\n",
    "\n",
    "df = pd.DataFrame(d)  # main df\n",
    "print(\"DataFrame Ready...\\n\")\n",
    "\n",
    "print(\"Field astype config\\n\")\n",
    "\n",
    "df['TICKET_ID']= df['TICKET_ID'].astype(int)\n",
    "df['FAILURE_COUNT']= df['FAILURE_COUNT'].astype(int)\n",
    "df['CREATED_ERROR_CODE']= df['CREATED_ERROR_CODE'].astype(int)\n",
    "df['TICKET_CREATION_DATE']= pd.to_datetime(df['TICKET_CREATION_DATE'])\n",
    "df['LAST_MODIFIED_DATE']= pd.to_datetime(df['LAST_MODIFIED_DATE'])\n",
    "df['TAT_WITH_EXCLUSION_IN_HOURS']= df['TAT_WITH_EXCLUSION_IN_HOURS'].astype(float)\n",
    "df['TAT_WITHOUT_EXCLUSION_IN_HOURS']= df['TAT_WITHOUT_EXCLUSION_IN_HOURS'].astype(float)\n",
    "df['EFFORT_IN_HOURS']= df['EFFORT_IN_HOURS'].astype(float)\n",
    "df['ASSIGNED_TEAM_ID']= df['ASSIGNED_TEAM_ID'].astype(int)\n",
    "\n",
    "df = df[['Owner', 'TICKET_ID', 'FAILURE_COUNT', 'TICKET_TYPE', 'SCRIPTNAME',\n",
    "       'INSTITUTION', 'CREATED_ERROR_CODE', 'STATUS', 'TAG_NAME',\n",
    "       'TICKET_CREATION_DATE', 'LAST_MODIFIED_DATE', 'PRODUCT_NAME',\n",
    "       'COUNTRY_CODE', 'TAT_WITH_EXCLUSION_IN_HOURS',\n",
    "       'TAT_WITHOUT_EXCLUSION_IN_HOURS', 'INCIDENT_NUMBER', 'Team',\n",
    "       'ASSIGNEDTO', 'EFFORT_IN_HOURS', 'SLA_MET', 'TICKET_DESC', 'manager',\n",
    "       'CHANNEL_TYPE_NAME', 'ERROR_CATEGORY', 'Error_Description'\n",
    "       , 'req_uri', 'rootCauseName', 'ticketRootCauseId','ASSIGNED_TEAM_ID']]\n",
    "\n",
    "con1 = vertica_python.connect(host='vertica.data-lake-prd.a.intuit.com', port=5433, dbname='IDEASHARED', user='douaknine', password='Logorn12')\n",
    "cursor = con1.cursor()\n",
    "\n",
    "cursor.execute(\"\"\"truncate table FDP_ANALYTICS_WS.fdp_dashboards_temp_with_SF;\"\"\")\n",
    "copy_str = \"\"\"COPY FDP_ANALYTICS_WS.fdp_dashboards_temp_with_SF(\n",
    "Owner,\n",
    "TICKET_ID,\n",
    "FAILURE_COUNT,\n",
    "TICKET_TYPE,\n",
    "SCRIPTNAME,\n",
    "INSTITUTION,\n",
    "CREATED_ERROR_CODE,\n",
    "STATUS,\n",
    "TAG_NAME,\n",
    "TICKET_CREATION_DATE,\n",
    "LAST_MODIFIED_DATE,\n",
    "PRODUCT_NAME,\n",
    "COUNTRY_CODE,\n",
    "TAT_WITH_EXCLUSION_IN_HOURS,\n",
    "TAT_WITHOUT_EXCLUSION_IN_HOURS,\n",
    "INCIDENT_NUMBER,\n",
    "Team,\n",
    "ASSIGNEDTO,\n",
    "EFFORT_IN_HOURS,\n",
    "SLA_MET,\n",
    "TICKET_DESC,\n",
    "manager,\n",
    "CHANNEL_TYPE_NAME,\n",
    "ERROR_CATEGORY,\n",
    "Error_Description,\n",
    "req_uri,\n",
    "rootCauseName,\n",
    "ticketRootCauseId,\n",
    "ASSIGNED_TEAM_ID)FROM STDIN DELIMITER ','\"\"\"\n",
    "\n",
    "stream = io.StringIO()\n",
    "df.to_csv(stream, sep=\",\",index=False, header=False)\n",
    "\n",
    "print(\"Stream init...\\n\")\n",
    "stream.seek(0) \n",
    "\n",
    "\n",
    "with con1.cursor() as cursor:\n",
    "    cursor.copy(copy_str,stream.getvalue())\n",
    "con1.commit()\n",
    "\n",
    "print(\"fdp_dashboards_temp_with_SF ...ok\\n\")\n",
    "\n",
    "cursor.execute(\"\"\"select count(case when a.TICKET_ID is null then 1 else null end) new_record,\n",
    "        count(case when b.TICKET_ID is null then 1 else null end) archived_record,\n",
    "        count(case when a.TICKET_ID is not null and b.TICKET_ID is not null then 1 else null end) overlap,\n",
    "        count(*) total_records\n",
    "from FDP_ANALYTICS_WS.fdp_dashboards_with_SF a full outer join\n",
    "        FDP_ANALYTICS_WS.fdp_dashboards_temp_with_SF b on a.TICKET_ID = b.TICKET_ID;\"\"\")\n",
    "\n",
    "\n",
    "q1row = cursor.fetchall()\n",
    "print('new_record - archived_record - overlap - total_records')\n",
    "print(q1row)\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "cursor.execute(\"\"\"drop table FDP_ANALYTICS_WS.fdp_dashboards_backup_with_SF;\"\"\")\n",
    "#Drop if right OK\n",
    "\n",
    "cursor.execute(\"\"\"select *\n",
    "into FDP_ANALYTICS_WS.fdp_dashboards_backup_with_SF\n",
    "from FDP_ANALYTICS_WS.fdp_dashboards_with_SF a\n",
    ";\"\"\")\n",
    "cursor.execute(\"\"\"delete from FDP_ANALYTICS_WS.fdp_dashboards_with_SF where TICKET_ID in \n",
    "(\n",
    "select distinct a.TICKET_ID\n",
    "from FDP_ANALYTICS_WS.fdp_dashboards_with_SF a full outer join\n",
    "        FDP_ANALYTICS_WS.fdp_dashboards_temp_with_SF b on a.TICKET_ID = b.TICKET_ID\n",
    "where a.TICKET_ID is not null and b.TICKET_ID is not null\n",
    ");\"\"\")\n",
    "cursor.execute(\"\"\"insert into FDP_ANALYTICS_WS.fdp_dashboards_with_SF\n",
    "select *\n",
    "from FDP_ANALYTICS_WS.fdp_dashboards_temp_with_SF\n",
    ";\"\"\")\n",
    "con1.commit()\n",
    "\n",
    "print(\"fdp_dashboards_backup_with_SF ...ok\\n\")\n",
    "print(\"fdp_dashboards_with_SF ...ok\\n\")\n",
    "\n",
    "\n",
    "print(\"DONE :)\\n\")\n",
    "\n",
    "print('Refresh Tableau Server...\\n')\n",
    "\n",
    "\n",
    "tableau_auth = TSC.TableauAuth('douaknine', 'Logorn12', site_id = 'FDS')\n",
    "server = TSC.Server('https://tableau.a.intuit.com',use_server_version = True)\n",
    "server.auth.sign_in(tableau_auth)\n",
    "print('connection made\\n')\n",
    "print(server.version)\n",
    "server.workbooks.refresh(workbook_id='ac076c97-73bb-4c80-95b3-924494e12c44')\n",
    "server.workbooks.refresh(workbook_id='37aff87d-4dca-4f23-bba5-975f69daefe4')\n",
    "server.workbooks.refresh(workbook_id='bf33b515-30c6-46df-9019-6940e47b81f8')\n",
    "server.workbooks.refresh(workbook_id='02451361-b8ac-4542-a3c0-dca6e054e202')\n",
    "print('refresh complete\\n')\n",
    "\n",
    "server.auth.sign_out()\n",
    "print('connection closed\\n')\n",
    "\n",
    "\n",
    "\n",
    "print(\"DONE :)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
